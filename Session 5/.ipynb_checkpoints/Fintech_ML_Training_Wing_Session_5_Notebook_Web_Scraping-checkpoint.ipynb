{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxO--psQCnvf"
   },
   "source": [
    "## Week 5 | Web Scraping using Urllib and Beautiful Soup\n",
    "### Learning Objectives\n",
    "At the end of this lesson, you will be able to:\n",
    "\n",
    "- Understand basic HTML\n",
    "- Retrieve `html` string of a website\n",
    "- Parse `html` string using `Beautiful Soup` to create a `Soup` data structure\n",
    "- Apply basic CSS selectors to find elements of a `Soup` data structure\n",
    "- Find children of an element using a `Soup` data structure\n",
    "- Find properties of an element using a `Soup` data structure\n",
    "- Download a pdf using the link found from a `Soup` data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvp8TLf6Tlcl"
   },
   "source": [
    "## Sending a HTTP Request and getting a Response\n",
    "- Use `urllib.request.Request` to specify the URL and headers, which is indicating the user agent as google chrome.\n",
    "- Use `urllib.request.urlopen(req)` to send a request to the server.\n",
    "- html contains the website in HTML format. It is a long string representing the various tags. Convert it to an object with BeautifulSoup datatype so extraction of tags can be performed.\n",
    "- Use the prettify() function from BeautifulSoup to see the HTML with indentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Kt1n7IaRYe8m"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup # HTML data structure\n",
    "import urllib.request\n",
    "\n",
    "def getSoup(URL):\n",
    "  req = urllib.request.Request(URL, headers={'User-Agent':' Chrome'})\n",
    "  html = urllib.request.urlopen(req).read().decode(\"utf8\")\n",
    "  soup = BeautifulSoup(html, \"html.parser\")\n",
    "  soup.prettify()\n",
    "\n",
    "  return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "enktQ_SU4FI9"
   },
   "outputs": [],
   "source": [
    "base = 'https://www.testpapersfree.com/'\n",
    "soup = getSoup(base)\n",
    "# soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGq0BhluUR30"
   },
   "source": [
    "## Element Selection using `find()`\n",
    "\n",
    "- Use `soup.find()` to get the tags you are interested in. Details about `find()` can be found [here](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find)\n",
    "- Specify the `id`, `class`, etc in the attrs. \n",
    "- `soup.find()` only returns the first element found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Td1PFrsNUriX"
   },
   "outputs": [],
   "source": [
    "# Finds the first div element that has id as main\n",
    "main_div = soup.find('div', attrs={'id' : 'main'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zsULsYtbVlFK"
   },
   "outputs": [],
   "source": [
    "# Finds the first div element that has class as posts.\n",
    "class_div = soup.find('div', attrs={'class' : 'posts'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-JTBKvJbVsH5"
   },
   "outputs": [],
   "source": [
    "# All attrs must match. In this case, none matches so no result is returned.\n",
    "no_div = soup.find('div', attrs={'class' : 'posts', 'id' : 'main'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fZ3EtIAWGO7"
   },
   "source": [
    "## Multiple Element Selection using `find_all()`\n",
    "\n",
    "- There are multiple `a` tags in the blog post. To find all `a` tags, use the `find_all()` function. Take note that this will return a `ResultSet`, which is a collection of `Tag`s. More information about `find_all()` can be found [here](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#calling-a-tag-is-like-calling-find-all).\n",
    "- The returned object can be treated as a list and iterated through using a for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-iWTTbrEWV9f"
   },
   "outputs": [],
   "source": [
    "# Finds ALL `a` elements\n",
    "a_elements = soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-GDTS5TIWlBt"
   },
   "outputs": [],
   "source": [
    "# Iterate through this ResultSet like a list\n",
    "for a in a_elements:\n",
    "    # print(a.text)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcIogYceXJGc"
   },
   "source": [
    "## Handling an element\n",
    "\n",
    "- `<a href=\"show.php?testpaperid=87490\">P6 Chinese 2020 Prelims - Anglo Chinese</a>`\n",
    "- Select what is between the <a>My Text</a> using `testpaper_element.text`\n",
    "- Select the `href` attribute value using [] just like a dictionary, using `testpaper_element[\"href\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "zEzVvNLZKDPo",
    "outputId": "a49b4352-3c2d-403b-b149-1f9cd73a248f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P6 Chinese 2020 Prelims - Anglo Chinese\n",
      "show.php?testpaperid=87490\n"
     ]
    }
   ],
   "source": [
    "# Select the first `a` element that contains \"testpaper\"\n",
    "testpaper_element = soup.select('a[href*=\"testpaper\"]')[0]\n",
    "print(testpaper_element.text)\n",
    "print(testpaper_element[\"href\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VohtJxrAY41T"
   },
   "source": [
    "## Download a single PDF\n",
    "\n",
    "- Create a new `Soup` data structure by navigating to the link found earlier\n",
    "- Find the download button for the school\n",
    "- Get the .pdf link from the download button\n",
    "- Download the .pdf file using the requests module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "hojA42MMqnQL"
   },
   "outputs": [],
   "source": [
    "# Create a new soup based on the new URL\n",
    "newSoup = getSoup(base + 'show.php?testpaperid=87490')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "kZwQb8vHvdOB"
   },
   "outputs": [],
   "source": [
    "# Select all the a elements with \"pdf\" (Download button)\n",
    "downloadButton = newSoup.select('a[href$=\".pdf\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "0Ntvf1uT2cjr",
    "outputId": "f350e708-d1b8-47ce-c393-8e60cd383705"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.testpapersfree.com/pdfs/P6_Chinese_SA2_2020_ACS_Exam_Papers.pdf'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the download link as string\n",
    "pdfLink = base + downloadButton[0]['href']\n",
    "pdfLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "egNq8__W3OFm"
   },
   "outputs": [],
   "source": [
    "# Making a directory for pdfs\n",
    "import os\n",
    "if not os.path.exists(\"pdfs\"):\n",
    "  os.mkdir(\"pdfs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Nm48J0ANxhsa"
   },
   "outputs": [],
   "source": [
    "# Download the pdf to pdfs directroy\n",
    "r = requests.get(pdfLink, stream=True)\n",
    "file_name = downloadButton[0]['href']\n",
    "\n",
    "with open(file_name, 'wb') as f:\n",
    "  f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QH9jOdTe7YFH"
   },
   "source": [
    "## Download multiple PDFs\n",
    "- Now, let us combine the code above to download multiple PDFs at once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "V9xNCFnj7kPE"
   },
   "outputs": [],
   "source": [
    "def download_pdfs(base):\n",
    "  soup = getSoup(base)\n",
    "  for link_element in soup.select('a[href*=\"testpaper\"]'):\n",
    "    link = link_element[\"href\"]\n",
    "    newSoup = getSoup(base + link)\n",
    "    downloadButton = newSoup.select('a[href$=\".pdf\"]')\n",
    "    print(downloadButton)\n",
    "    pdfLink = base + downloadButton[0]['href']\n",
    "    r = requests.get(pdfLink, stream=True)\n",
    "    file_name = downloadButton[0]['href']\n",
    "\n",
    "    with open(file_name, 'wb') as f:\n",
    "      f.write(r.content)\n",
    "  return \n",
    "\n",
    "#download_pdfs(base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7YqIXEgi5VX"
   },
   "source": [
    "## Activity Time!\n",
    "- Let us put what you have learnt into practice!\n",
    "- You will now be split into breakout rooms of 4-5 people! This discussion session will last for about 20 minutes. \n",
    "- These questions are based on https://www.testpapersfree.com/\n",
    "- The files should be downloaded through web scraping (code) only.\n",
    "- The code is only allowed to start at the link above, and not sublinks.\n",
    "- Each group will send a representative to answer these questions:\n",
    "1. Download P2-Chinese-2014-SA2-Tao-Nan.pdf (Difficulty: Easy, Hint: It is one of the pdfs available on the main website)\n",
    "2. Download P6_Maths_SA2_2018_Raffles_Exam_Paper.pdf (Difficulty: Challenging)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check out the answers at the github page after the training session!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Fintech ML Training Wing Session 5 Notebook-Web Scraping.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
