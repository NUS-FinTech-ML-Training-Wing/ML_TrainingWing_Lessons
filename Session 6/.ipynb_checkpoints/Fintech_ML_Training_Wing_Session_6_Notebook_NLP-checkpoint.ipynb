{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fintech_ML_Training_Wing_Session_6_Notebook_NLP\n",
    "<hr>\n",
    "\n",
    "## Learning Objectives\n",
    "General objectives:\n",
    "\n",
    "- define **corpus**, **documents** and **terms** as part of the study of Natural Language Processing\n",
    "\n",
    "- define **tokenisation** as breaking a document into terms\n",
    "\n",
    "- understand the definition of **root form** of a word for verbs and nouns\n",
    "\n",
    "- identify **stemming** as a way to find the root form of a word\n",
    "\n",
    "- list tags from a Part-of-Speech tagger related to verbs, nouns, prepositions, adverbs, adjectives and d\n",
    "\n",
    "- interpret the output of a **Part-of-Speech tagger**\n",
    "\n",
    "- define a **lemma** as a word that can be found in a dictionary\n",
    "\n",
    "- identify lemmatisation a way to find the root form of a word\n",
    "\n",
    "- learn how to use **stop words** to filter out terms in a document that is not meaningful\n",
    "\n",
    "- use **Jaccard Similarity** to find similar texts\n",
    "\n",
    "- perform **sentiment analysis** using the `SentimentIntensityAnalyzer`\n",
    "\n",
    "\n",
    "\n",
    "Guidelines:\n",
    "\n",
    "- use `word_tokenize` from `nltk.tokenize` to break a document into a list of words\n",
    "\n",
    "- use `PorterStemmer`'s `stem` from `nltk.stem` to perform stemming of words\n",
    "\n",
    "- use `pos_tag` from `nltk` to perform Part-of-Speech (POS) tagging of a sentence\n",
    "\n",
    "- given a word and its POS tag, use `WordNetLemmatizer`'s `lemmatize` from `nltk.stem` to find its corresponding lemma \n",
    "\n",
    "- retrieve a list of stopwords defined in `stopwords.words()` from `nltk.corpus`\n",
    "\n",
    "- extend the existing implementation of Jaccard Similarity to find similar texts\n",
    "\n",
    "- find additional corpora in the `nltk` library\n",
    "\n",
    "- further understanding of `CountVectorizer`\n",
    "\n",
    "### Datasets Required for this Self-Study\n",
    "1. `songs-100.csv`\n",
    "\n",
    "2. `loans-descs-1k.csv`\n",
    "\n",
    "Adapted from Hackwagon DS102"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:02:18.799691Z",
     "start_time": "2018-10-17T17:02:17.802382Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:02:18.807204Z",
     "start_time": "2018-10-17T17:02:18.802640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading corpora...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\simsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\simsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\simsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\simsh\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpora download complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\simsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Use this cell to download all the required corpora first. Then, comment out this block of code.\n",
    "print(\"Downloading corpora...\")    \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "print(\"Corpora download complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:02:18.815259Z",
     "start_time": "2018-10-17T17:02:18.810868Z"
    }
   },
   "outputs": [],
   "source": [
    "# If you are running this for the first time, use the previous cell to download all \n",
    "# the corpora before starting.\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus, Documents and Terms\n",
    "\n",
    "In lingustics (the study of language), a **corpus** is a collection of texts, represented by documents. A **document** contains multiple words and when strung together, produce meaning. Each word is called a **term**. \n",
    "\n",
    "Consider the following corpus of 4 documents from the 100 Song Titles dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:02:18.824296Z",
     "start_time": "2018-10-17T17:02:18.819468Z"
    }
   },
   "outputs": [],
   "source": [
    "song_titles = ['shape of you', 'paris', 'scared to be lonely', \n",
    "               'symphony feat zara larsson',]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each <u>song title is a document</u>. The <u>collection of song titles forms the corpus</u>. The first document `shape of you` has <u>3 terms</u>. The second document `paris` has <u>1 term</u>. The third document `scared to be lonely` has <u>4 terms</u>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now read from `songs-100.csv`, a CSV file which, in this case is our corpus. There are $100$ documents in the song titles corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:02:18.850706Z",
     "start_time": "2018-10-17T17:02:18.827522Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    100\n",
       "name          100\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read from 'songs-100.csv' into song_titles_df\n",
    "#\n",
    "song_titles_df = pd.read_csv('songs-100.csv')\n",
    "# Use count() to find the number of documents in the corpus.\n",
    "#\n",
    "song_titles_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation using `nltk`\n",
    "\n",
    "The Natural Language Toolkit or `nltk` library is a very powerful library used for natural language processing. We will be using `nltk.tokenize.word_tokenize()` to perform tokenisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:02:18.866585Z",
     "start_time": "2018-10-17T17:02:18.853625Z"
    }
   },
   "outputs": [],
   "source": [
    "loan_descs_df = pd.read_csv('loans-descs-1k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:02:18.879967Z",
     "start_time": "2018-10-17T17:02:18.869845Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Borrower added on 02/14/14 > I am consolidating credit card debt incurred over three years ago and having a concrete end in sight is more motivating.  I am eagerly striving towards becoming completely debt free.<br>\n"
     ]
    }
   ],
   "source": [
    "# Convert the raw text into 2 sentences that can be used for processing\n",
    "s1 = loan_descs_df.loc[4]['desc']\n",
    "print(s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `re.sub()` to substitute the initial phrase with an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:02:18.892740Z",
     "start_time": "2018-10-17T17:02:18.883672Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Borrower added on 02/14/14 > I am consolidating credit card debt incurred over three years ago and having a concrete end in sight is more motivating.  I am eagerly striving towards becoming completely debt free.<br>\n",
      "\n",
      "I am consolidating credit card debt incurred over three years ago and having a concrete end in sight is more motivating.  I am eagerly striving towards becoming completely debt free.\n"
     ]
    }
   ],
   "source": [
    "print(s1)\n",
    "r = 'Borrower added on \\d+/\\d+/\\d+ >|<br>'\n",
    "s2 = re.sub(r, '', s1)\n",
    "s2 = s2.strip()\n",
    "print()\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tokenise a sentence, simply use `word_tokenize()` from the `nltk` library. This will convert the sentence into individual words AND special characters like full stops and commas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:02:18.922820Z",
     "start_time": "2018-10-17T17:02:18.897091Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Borrower', 'added', 'on', '02/14/14', '>', 'I', 'am', 'consolidating', 'credit', 'card', 'debt', 'incurred', 'over', 'three', 'years', 'ago', 'and', 'having', 'a', 'concrete', 'end', 'in', 'sight', 'is', 'more', 'motivating', '.', 'I', 'am', 'eagerly', 'striving', 'towards', 'becoming', 'completely', 'debt', 'free.', '<', 'br', '>']\n"
     ]
    }
   ],
   "source": [
    "# Use word_tokenize(string) to convert the string into a list of tokens.\n",
    "# Assign this to a new variable called ts1\n",
    "#\n",
    "ts1 = word_tokenize(s1)\n",
    "print(ts1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linguistics - The root form of a word (verbs & nouns)\n",
    "Stemming is one way to find the **root form of a word**. We will only limit our discussion to verbs (action words) and nouns (naming words). First, consider the following 3 sentences that use different forms of the word `watch`:\n",
    "\n",
    "- `Larry watches television.` (singular present tense)\n",
    "\n",
    "- `The children watch television.` (simple tense / plural, present tense)\n",
    "\n",
    "- `My son is watching television.` (present participle tense / present continuous tense)\n",
    "\n",
    "- `My mum watched television with me.` (past tense)\n",
    "\n",
    "The word `watch` exists in 4 different <u>forms of the **verb**</u> as they exist in different tenses. However, algorithms treat them as **separate words** during analysis. Hence, we need to find the root form of the verb so they can be treated as the same word during analysis as they have the same meaning, in this case `watch`. \n",
    "\n",
    "`nltk` implements the **Porter Stemmer** and you can find the reference for the rules [here](http://www.nltk.org/howto/stem.html). Use `stemmer = PorterStemmer()` and then use the `stem()` method for each word to get its root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:02:18.935065Z",
     "start_time": "2018-10-17T17:02:18.926493Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "larri\n",
      "watch\n",
      "televis\n",
      ".\n",
      "\n",
      "the\n",
      "children\n",
      "watch\n",
      "televis\n",
      ".\n",
      "\n",
      "My\n",
      "son\n",
      "is\n",
      "watch\n",
      "televis\n",
      ".\n",
      "\n",
      "My\n",
      "mum\n",
      "watch\n",
      "televis\n",
      "with\n",
      "me\n",
      ".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "ss_verbs = ['Larry watches television.', 'The children watch television.', \n",
    "       'My son is watching television.', 'My mum watched television with me.']\n",
    "\n",
    "for s in ss_verbs:\n",
    "    for st in word_tokenize(s):\n",
    "        print(stemmer.stem(st))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider the next 2 sentences:\n",
    "\n",
    "- `This is a very expensive vase.` (singular noun)\n",
    "\n",
    "- `The third floor in this mall sells vases.` (plural noun)\n",
    "\n",
    "Similarly, we need to find the root <u>form of the **noun**</u>, in this case `vase`. Although only differing in one letter, the ending `s`, algorithms treat them as distinct words. Hence, we need to find the root form of the noun so they can be treated as the same word as they refer to the same object in real life."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:02:18.946027Z",
     "start_time": "2018-10-17T17:02:18.938503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thi\n",
      "is\n",
      "a\n",
      "veri\n",
      "expens\n",
      "vase\n",
      ".\n",
      "\n",
      "the\n",
      "third\n",
      "floor\n",
      "in\n",
      "thi\n",
      "mall\n",
      "sell\n",
      "vase\n",
      ".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ss_nouns = ['This is a very expensive vase.', \n",
    "            'The third floor in this mall sells vases.', ]\n",
    "# Exercise: Iterate through the list of sentences. Tokenise each sentense using word_tokenize().\n",
    "# Then for every term, print out its stemmed form using stemmer.stem(term)\n",
    "#\n",
    "for s in ss_nouns:\n",
    "    for st in word_tokenize(s):\n",
    "        print(stemmer.stem(st))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Now, we apply the stemming step on the initial loans sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:02:18.957612Z",
     "start_time": "2018-10-17T17:02:18.949208Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Borrower', 'added', 'on', '02/14/14', '>', 'I', 'am', 'consolidating', 'credit', 'card', 'debt', 'incurred', 'over', 'three', 'years', 'ago', 'and', 'having', 'a', 'concrete', 'end', 'in', 'sight', 'is', 'more', 'motivating', '.', 'I', 'am', 'eagerly', 'striving', 'towards', 'becoming', 'completely', 'debt', 'free.', '<', 'br', '>']\n"
     ]
    }
   ],
   "source": [
    "# Just to refresh our memory...\n",
    "print(ts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:02:18.970016Z",
     "start_time": "2018-10-17T17:02:18.961432Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['borrow', 'ad', 'on', '02/14/14', '>', 'I', 'am', 'consolid', 'credit', 'card', 'debt', 'incur', 'over', 'three', 'year', 'ago', 'and', 'have', 'a', 'concret', 'end', 'in', 'sight', 'is', 'more', 'motiv', '.', 'I', 'am', 'eagerli', 'strive', 'toward', 'becom', 'complet', 'debt', 'free.', '<', 'br', '>']\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a stemmer\n",
    "#\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = []\n",
    "for t in ts1:\n",
    "    # Use stemmer.stem() to find the root form of the word\n",
    "    #\n",
    "    u = stemmer.stem(t)\n",
    "#    stemmed_words.append(t)    \n",
    "    stemmed_words.append(u)\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, some stemmed words are not valid english words. For example, `consolid` is not an English word. `motiv` and `eagerli` too. However, because of its relatively simple algorithm, some applications accept this form of the word and hence this algorithm is useful. Examples of implementations of stemming are in search engines as both the search term and text can be stemmed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows the original form of the sentence and the result after stemming for easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:02:18.981970Z",
     "start_time": "2018-10-17T17:02:18.973859Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Raw          Stemming   \n",
      "   --------------    ------------\n",
      "       Borrower            borrow\n",
      "          added                ad\n",
      "             on                on\n",
      "       02/14/14          02/14/14\n",
      "              >                 >\n",
      "              I                 I\n",
      "             am                am\n",
      "  consolidating          consolid\n",
      "         credit            credit\n",
      "           card              card\n",
      "           debt              debt\n",
      "       incurred             incur\n",
      "           over              over\n",
      "          three             three\n",
      "          years              year\n",
      "            ago               ago\n",
      "            and               and\n",
      "         having              have\n",
      "              a                 a\n",
      "       concrete           concret\n",
      "            end               end\n",
      "             in                in\n",
      "          sight             sight\n",
      "             is                is\n",
      "           more              more\n",
      "     motivating             motiv\n",
      "              .                 .\n",
      "              I                 I\n",
      "             am                am\n",
      "        eagerly           eagerli\n",
      "       striving            strive\n",
      "        towards            toward\n",
      "       becoming             becom\n",
      "     completely           complet\n",
      "           debt              debt\n",
      "          free.             free.\n",
      "              <                 <\n",
      "             br                br\n"
     ]
    }
   ],
   "source": [
    "print(\"%15s   %15s   \" % (\"Raw\", \"Stemming\"))\n",
    "print(\"%15s-- %15s\" % (\"------------\", \"------------\"))\n",
    "for i in range(0, len(stemmed_words)-1):\n",
    "    print (\"%15s   %15s\" % (ts1[i], stemmed_words[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider another algorithm to find the root form of a word, called **Lemmatisation**. Before we start talking about Lemmatisation, we need to first understand **Part-of-Speech (POS) Tagging**. \n",
    "\n",
    "### Part-of-Speech (POS) Tagging\n",
    "\n",
    "POS tagging is a way to group a word into its **class**. Commonly, a word and tag pair is represented as a tuple. We will use one of `nltk`'s tagged corpora, in particular the *Penn Treebank Project* to help us tag newly discovered words. To find the POS tag of a word, use `nltk.pos_tag(word_tokens)`.\n",
    "\n",
    "Notice that the resulting value consists of many tuples. The first element in the tuple is the original word from the sentence and the second element is the assigned POS tag. Refer to this [link](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) to understand the meaning of each tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T15:46:41.865423Z",
     "start_time": "2018-10-17T15:46:41.654792Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Borrower', 'NNP'), ('added', 'VBD'), ('on', 'IN'), ('02/14/14', 'CD'), ('>', 'NN'), ('I', 'PRP'), ('am', 'VBP'), ('consolidating', 'VBG'), ('credit', 'NN'), ('card', 'NN'), ('debt', 'NN'), ('incurred', 'VBN'), ('over', 'IN'), ('three', 'CD'), ('years', 'NNS'), ('ago', 'RB'), ('and', 'CC'), ('having', 'VBG'), ('a', 'DT'), ('concrete', 'JJ'), ('end', 'NN'), ('in', 'IN'), ('sight', 'NN'), ('is', 'VBZ'), ('more', 'RBR'), ('motivating', 'JJ'), ('.', '.'), ('I', 'PRP'), ('am', 'VBP'), ('eagerly', 'RB'), ('striving', 'JJ'), ('towards', 'NNS'), ('becoming', 'VBG'), ('completely', 'RB'), ('debt', 'NN'), ('free.', 'NN'), ('<', 'NNP'), ('br', 'NN'), ('>', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "tagged_words_by_treebank = nltk.pos_tag(ts1)\n",
    "print(tagged_words_by_treebank)\n",
    "# Tags to be aware of: PRP, VBP, VBG, NN, VBN, NNS+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Borrower', 'NNP'), ('added', 'VBD'), ('on', 'IN'), ('02/14/14', 'CD'), ('>', 'NN'), ('I', 'PRP'), ('am', 'VBP'), ('consolidating', 'VBG'), ('credit', 'NN'), ('card', 'NN'), ('debt', 'NN'), ('incurred', 'VBN'), ('over', 'IN'), ('three', 'CD'), ('years', 'NNS'), ('ago', 'RB'), ('and', 'CC'), ('having', 'VBG'), ('a', 'DT'), ('concrete', 'JJ'), ('end', 'NN'), ('in', 'IN'), ('sight', 'NN'), ('is', 'VBZ'), ('more', 'RBR'), ('motivating', 'JJ'), ('.', '.'), ('I', 'PRP'), ('am', 'VBP'), ('eagerly', 'RB'), ('striving', 'JJ'), ('towards', 'NNS'), ('becoming', 'VBG'), ('completely', 'RB'), ('debt', 'NN'), ('free.', 'NN'), ('<', 'NNP'), ('br', 'NN'), ('>', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "tagged_words_by_treebank2 = nltk.pos_tag(word_tokenize(\"Mary leaves the room\"))\n",
    "print(tagged_words_by_treebank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Borrower', 'NNP'), ('added', 'VBD'), ('on', 'IN'), ('02/14/14', 'CD'), ('>', 'NN'), ('I', 'PRP'), ('am', 'VBP'), ('consolidating', 'VBG'), ('credit', 'NN'), ('card', 'NN'), ('debt', 'NN'), ('incurred', 'VBN'), ('over', 'IN'), ('three', 'CD'), ('years', 'NNS'), ('ago', 'RB'), ('and', 'CC'), ('having', 'VBG'), ('a', 'DT'), ('concrete', 'JJ'), ('end', 'NN'), ('in', 'IN'), ('sight', 'NN'), ('is', 'VBZ'), ('more', 'RBR'), ('motivating', 'JJ'), ('.', '.'), ('I', 'PRP'), ('am', 'VBP'), ('eagerly', 'RB'), ('striving', 'JJ'), ('towards', 'NNS'), ('becoming', 'VBG'), ('completely', 'RB'), ('debt', 'NN'), ('free.', 'NN'), ('<', 'NNP'), ('br', 'NN'), ('>', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "tagged_words_by_treebank3 = nltk.pos_tag(word_tokenize(\"Dew drops fall from the leaves\"))\n",
    "print(tagged_words_by_treebank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the first letter of the tag represent similar classes. In particular, \n",
    "\n",
    "- the pattern `N[A-Z]+` represents nouns and \n",
    "\n",
    "- the pattern `V[A-Z]+` represent verbs\n",
    "\n",
    "Hence, we can take the first character and convert it to lower case. This first letter of the tag will be used for Lemmatisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T15:49:01.854704Z",
     "start_time": "2018-10-17T15:49:01.849710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Borrower', 'n'), ('added', 'v'), ('on', 'i'), ('02/14/14', 'c'), ('>', 'n'), ('I', 'p'), ('am', 'v'), ('consolidating', 'v'), ('credit', 'n'), ('card', 'n'), ('debt', 'n'), ('incurred', 'v'), ('over', 'i'), ('three', 'c'), ('years', 'n'), ('ago', 'r'), ('and', 'c'), ('having', 'v'), ('a', 'd'), ('concrete', 'j'), ('end', 'n'), ('in', 'i'), ('sight', 'n'), ('is', 'v'), ('more', 'r'), ('motivating', 'j'), ('.', '.'), ('I', 'p'), ('am', 'v'), ('eagerly', 'r'), ('striving', 'j'), ('towards', 'n'), ('becoming', 'v'), ('completely', 'r'), ('debt', 'n'), ('free.', 'n'), ('<', 'n'), ('br', 'n'), ('>', 'n')]\n"
     ]
    }
   ],
   "source": [
    "tagged_words = []\n",
    "for twt in tagged_words_by_treebank:\n",
    "    # Get the first element of the tuple, and the first letter of the second element\n",
    "    # of the tuple.\n",
    "    tagged_words.append((twt[0], twt[1][0].lower()))\n",
    "print(tagged_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatisation\n",
    "\n",
    "A **Lemma** is a word found in the dictionary. Hence, you can think of them as the root form of a word. Given a word and its corresponding tag, we can find the word's root form in English. This will be easier for human interpretation. Use `WordNetLemmatizer.lemmatize(term, pos=tag)` to find the root form of the word given the source word and its associated POS tag.\n",
    "\n",
    "Note that if the POS tag cannot be found, a `KeyError` will be thrown. For example, the first word will have the following result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T15:51:49.603028Z",
     "start_time": "2018-10-17T15:51:49.599705Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# lemmatiser = WordNetLemmatizer()\n",
    "# lemmatiser.lemmatize('I', pos='p') # Uncomment this line to see the KeyError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, wrap the Lemmatisation step into `try...catch` block so the program continues even if a `KeyError` is encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T15:52:50.621875Z",
     "start_time": "2018-10-17T15:52:50.614863Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError: on\n",
      "KeyError: 02/14/14\n",
      "KeyError: I\n",
      "KeyError: over\n",
      "KeyError: three\n",
      "KeyError: and\n",
      "KeyError: a\n",
      "KeyError: concrete\n",
      "KeyError: in\n",
      "KeyError: motivating\n",
      "KeyError: .\n",
      "KeyError: I\n",
      "KeyError: striving\n",
      "['Borrower', 'add', 'on', '02/14/14', '>', 'I', 'be', 'consolidate', 'credit', 'card', 'debt', 'incur', 'over', 'three', 'year', 'ago', 'and', 'have', 'a', 'concrete', 'end', 'in', 'sight', 'be', 'more', 'motivating', '.', 'I', 'be', 'eagerly', 'striving', 'towards', 'become', 'completely', 'debt', 'free.', '<', 'br', '>']\n"
     ]
    }
   ],
   "source": [
    "lemmatiser = WordNetLemmatizer()\n",
    "lemmed_words = []\n",
    "for tw_pair in tagged_words:\n",
    "    tw_word, tw_tag = tw_pair[0], tw_pair[1]\n",
    "    lemm_word = tw_word\n",
    "    try:\n",
    "        lemm_word = lemmatiser.lemmatize(tw_word, pos=tw_tag)\n",
    "    except KeyError:\n",
    "        # If an error is thrown, then the word is assumed to be in its root form.\n",
    "        print(\"KeyError: \" + tw_word)\n",
    "        pass\n",
    "\n",
    "    lemmed_words.append(lemm_word)\n",
    "print(lemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows the original form of the sentence, the result after stemming and the result after lemmatisation for easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T15:46:44.279163Z",
     "start_time": "2018-10-17T15:46:44.270061Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Raw          Stemming     Lemmatisation\n",
      "   --------------    --------------  --------------\n",
      "       Borrower            borrow          Borrower\n",
      "          added                ad               add\n",
      "             on                on                on\n",
      "       02/14/14          02/14/14          02/14/14\n",
      "              >                 >                 >\n",
      "              I                 I                 I\n",
      "             am                am                be\n",
      "  consolidating          consolid       consolidate\n",
      "         credit            credit            credit\n",
      "           card              card              card\n",
      "           debt              debt              debt\n",
      "       incurred             incur             incur\n",
      "           over              over              over\n",
      "          three             three             three\n",
      "          years              year              year\n",
      "            ago               ago               ago\n",
      "            and               and               and\n",
      "         having              have              have\n",
      "              a                 a                 a\n",
      "       concrete           concret          concrete\n",
      "            end               end               end\n",
      "             in                in                in\n",
      "          sight             sight             sight\n",
      "             is                is                be\n",
      "           more              more              more\n",
      "     motivating             motiv        motivating\n",
      "              .                 .                 .\n",
      "              I                 I                 I\n",
      "             am                am                be\n",
      "        eagerly           eagerli           eagerly\n",
      "       striving            strive          striving\n",
      "        towards            toward           towards\n",
      "       becoming             becom            become\n",
      "     completely           complet        completely\n",
      "           debt              debt              debt\n",
      "          free.             free.             free.\n",
      "              <                 <                 <\n",
      "             br                br                br\n"
     ]
    }
   ],
   "source": [
    "print(\"%15s   %15s   %15s\" % (\"Raw\", \"Stemming\", \"Lemmatisation\"))\n",
    "print(\"%15s-- %15s-- %15s\" % (\"------------\", \"------------\", \"--------------\"))\n",
    "for i in range(0, len(stemmed_words)-1):\n",
    "    print (\"%15s   %15s   %15s\" % (ts1[i], stemmed_words[i], lemmed_words[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words\n",
    "\n",
    "Finally, before performing analysis, remove **stop words** from the sentence. A stop word is a word that usually appears in many texts, and hence do not hold any meaning. In signal processing language, this is referred to as <u>noise</u>. Refer to this [Github link](https://gist.github.com/sebleier/554280) for the list of stop words from `nltk`. `nltk.corpus.stopwords.words()` contains the list of stop words and if the word exists in them, ignore them.\n",
    "\n",
    "Recall that \n",
    "\n",
    "```python\n",
    "    word in wordlist\n",
    "``` \n",
    "is used to check if a word is in a list. It returns `True` if the word is found and `False` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:02:19.331911Z",
     "start_time": "2018-10-17T17:02:18.984966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['borrow', 'ad', 'on', '02/14/14', '>', 'I', 'am', 'consolid', 'credit', 'card', 'debt', 'incur', 'over', 'three', 'year', 'ago', 'and', 'have', 'a', 'concret', 'end', 'in', 'sight', 'is', 'more', 'motiv', '.', 'I', 'am', 'eagerli', 'strive', 'toward', 'becom', 'complet', 'debt', 'free.', '<', 'br', '>']\n",
      "\n",
      "['borrow', 'ad', '02/14/14', '>', 'I', 'consolid', 'credit', 'card', 'debt', 'incur', 'three', 'year', 'ago', 'concret', 'end', 'sight', 'motiv', '.', 'I', 'eagerli', 'strive', 'toward', 'becom', 'complet', 'debt', 'free.', '<', 'br', '>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_list_of_words = []\n",
    "for l in stemmed_words:\n",
    "    # Use not in stopwords.words('english') to check if the word \n",
    "    # is a stop word. If it isn't, then append to the final_list_of_words.\n",
    "    if l not in stopwords.words('english'):\n",
    "        final_list_of_words.append(l)\n",
    "print(stemmed_words)\n",
    "print()\n",
    "print(final_list_of_words)\n",
    "len(stemmed_words) - len(final_list_of_words)\n",
    "# 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of pipeline \n",
    "- Convert the sentence to lowercase\n",
    "- Remove all special characters according to the pattern `[.®'&$’\\\"\\-()]`\n",
    "- Perform tokenisation, followed by stemming of your selected sentence\n",
    "- Remove stop words from the list of stemmed words\n",
    "\n",
    "Note: You can use `'''` to specify a multi-line string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:02:19.340917Z",
     "start_time": "2018-10-17T17:02:19.335892Z"
    }
   },
   "outputs": [],
   "source": [
    "s2 = '''I really need to consolidate my credit card debt so that I can become debt free. \n",
    "The interest is killing me and I'm just not getting anywhere with the balances. Help!'''\n",
    "\n",
    "s3 = '''Hello, I just closed on the house of my dreams and I would like to \n",
    "use this loan to pay off my high interest credit cards and build a deck on my home.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:02:19.351080Z",
     "start_time": "2018-10-17T17:02:19.344806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, i just closed on the house of my dreams and i would like to \n",
      "use this loan to pay off my high interest credit cards and build a deck on my home.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Convert to lower case using lower()\n",
    "#\n",
    "s2_1 = s3.lower()\n",
    "print(s2_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:02:19.362468Z",
     "start_time": "2018-10-17T17:02:19.354561Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, i just closed on the house of my dreams and i would like to \n",
      "use this loan to pay off my high interest credit cards and build a deck on my home \n"
     ]
    }
   ],
   "source": [
    "# Step 2: Perform regex substitution to remove special characters.\n",
    "#\n",
    "s2_2 = re.sub(\"[.®'&$’\\\"\\-()]\", \" \", s2_1)\n",
    "print(s2_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:02:19.376047Z",
     "start_time": "2018-10-17T17:02:19.366111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', 'i', 'just', 'closed', 'on', 'the', 'house', 'of', 'my', 'dreams', 'and', 'i', 'would', 'like', 'to', 'use', 'this', 'loan', 'to', 'pay', 'off', 'my', 'high', 'interest', 'credit', 'cards', 'and', 'build', 'a', 'deck', 'on', 'my', 'home']\n"
     ]
    }
   ],
   "source": [
    "# Step 3: use word_tokenize() to tokenise the sentence and get a list of terms.\n",
    "#\n",
    "s2_3 = nltk.word_tokenize(s2_2)\n",
    "print(s2_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:02:19.388466Z",
     "start_time": "2018-10-17T17:02:19.380733Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', 'i', 'just', 'close', 'on', 'the', 'hous', 'of', 'my', 'dream', 'and', 'i', 'would', 'like', 'to', 'use', 'thi', 'loan', 'to', 'pay', 'off', 'my', 'high', 'interest', 'credit', 'card', 'and', 'build', 'a', 'deck', 'on', 'my', 'home']\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Use stemmer.stem() to get the list of stemmed terms.\n",
    "#\n",
    "st = PorterStemmer()\n",
    "s2_4 = [st.stem(s) for s in s2_3]\n",
    "print(s2_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:02:19.411191Z",
     "start_time": "2018-10-17T17:02:19.391491Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 5: Remove stop words. Remove the word if it appears in stopwords.words('english')\n",
    "#\n",
    "s2_5 = [s for s in s2_4 if s not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:02:19.419384Z",
     "start_time": "2018-10-17T17:02:19.414590Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', 'close', 'hous', 'dream', 'would', 'like', 'use', 'thi', 'loan', 'pay', 'high', 'interest', 'credit', 'card', 'build', 'deck', 'home']\n"
     ]
    }
   ],
   "source": [
    "# Finally, print() the sentence.\n",
    "#\n",
    "print(s2_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T07:57:39.864532Z",
     "start_time": "2018-11-08T07:57:37.844186Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "#If you are running this for the first time, use the next cell to download all the corpora first\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T07:57:39.874009Z",
     "start_time": "2018-11-08T07:57:39.867355Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download the VADER list of words / lexicon\n",
    "# nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard Similarity\n",
    "\n",
    "Jaccard Similarity is used to show how similar two documents are. Given two documents, $A$ and $B$, the Jaccard Similarity Score is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{Jaccard Similarity Score} = \\frac{A\\cap B}{A \\cup B}\n",
    "$$\n",
    "\n",
    "Simply put, the numerator is the number of words that **are common across both documents** and the denominator is the **total number of words in both documents**. Keep in mind that the words here refer to **unique words**.\n",
    "\n",
    "The function below, `calculate_jaccard_score` will return the similarity score of two documents, `d1` and `d2`. It uses list comprehensions and the documenation for that can be found [here](https://docs.python.org/3/tutorial/datastructures.html). Also, find out more about how multiple variables can be declared in the same line [here](https://docs.python.org/3.6/tutorial/datastructures.html#tuples-and-sequences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T07:57:39.888022Z",
     "start_time": "2018-11-08T07:57:39.879799Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_jaccard_score(d1, d2):\n",
    "    set_a, set_b = set(d1), set(d2)\n",
    "    return len(set_a & set_b) / len(set_a | set_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T07:57:39.900007Z",
     "start_time": "2018-11-08T07:57:39.891911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19047619047619047\n",
      "0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a', 'c', 'd', 'r'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = 'I would like to consolidate a few of my higher interest rate credit cards.'.split()\n",
    "s2 = 'this loan is to consolidate credit card debt and pay of debt'.split()\n",
    "s3 = 'card'\n",
    "s4 = 'Cards'\n",
    "print(calculate_jaccard_score(s1, s2))\n",
    "print(calculate_jaccard_score(s3, s4))\n",
    "set(s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fuzzywuzzy in c:\\programdata\\anaconda3\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: python-Levenshtein in c:\\programdata\\anaconda3\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from python-Levenshtein) (41.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install fuzzywuzzy\n",
    "!pip install python-Levenshtein\n",
    "\n",
    "from fuzzywuzzy import fuzz \n",
    "from fuzzywuzzy import process "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FuzzyWuzzy is a library of Python which is used for string matching. Fuzzy string matching is the process of finding strings that match a given pattern. Basically it uses  Levenshtein Distance to calculate the differences between sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.WRatio('card', 'Cards') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis with `nltk`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nltk` library has a sentiment analyser. It uses the VADER method or **Valence Aware Dictionary for\n",
    "sEntiment Reasoning**. It is a lexicon (vocabulary) of words and their relative sentiment strength. For example:\n",
    "    \n",
    "- `Good` has a positive but weak score, while `Excellent` scores more\n",
    "- `Bad` has a negative but weaks score, while `Tragedy` scores more\n",
    "\n",
    "Use `sid.polarity_scores(t)` to find the sentiment of a text. Then, use the `compound` value to determine the overall score. Note that `compound` give a (normalised) value from $-1$ to $1$, and hence a positive number is good sentiment while a negative number is bad sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T07:57:39.943405Z",
     "start_time": "2018-11-08T07:57:39.905303Z"
    }
   },
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how the sentiment scores change based on the sentiment of a movie review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T07:57:39.955545Z",
     "start_time": "2018-11-08T07:57:39.946867Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.754, 'pos': 0.246, 'compound': 0.5563}\n",
      "0.5563\n"
     ]
    }
   ],
   "source": [
    "# This is an example of a positive review, showing positive sentiment.\n",
    "review_1 = \"\"\"I thoroughly enjoyed this movie because there was a genuine sincerity in the acting.\"\"\"\n",
    "ss = sid.polarity_scores(review_1)\n",
    "print(ss)\n",
    "print(ss['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T07:57:39.967198Z",
     "start_time": "2018-11-08T07:57:39.959509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.326, 'neu': 0.503, 'pos': 0.171, 'compound': -0.3025}\n",
      "-0.3025\n"
     ]
    }
   ],
   "source": [
    "review_2 = \"I found it really boring and silly.\"\n",
    "ss2 = sid.polarity_scores(review_2)\n",
    "print(ss2)\n",
    "print(ss2['compound'])\n",
    "# This is an example of a negative review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T07:57:39.982925Z",
     "start_time": "2018-11-08T07:57:39.972474Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.381, 'neu': 0.309, 'pos': 0.309, 'compound': -0.1779}\n",
      "-0.1779\n"
     ]
    }
   ],
   "source": [
    "review_3 = \"My personal favorite horror film.\"\n",
    "ss3 = sid.polarity_scores(review_3)\n",
    "print(ss3)\n",
    "print(ss3['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T07:57:39.864532Z",
     "start_time": "2018-11-08T07:57:37.844186Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "#If you are running this for the first time, use the next cell to download all the corpora first\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert a Document to a Vector using `CountVectorizer`\n",
    "\n",
    "The following are the words in the corpus in the Naïve Bayes Classification example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:13:58.669626Z",
     "start_time": "2018-10-17T17:13:58.664317Z"
    }
   },
   "outputs": [],
   "source": [
    "docs_as_s = ['enjoy like', \n",
    "             'enjoy funny happy', \n",
    "             'hate boring like', \n",
    "             'like happy', \n",
    "             'boring dull']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fit_transform()` will first fit the dataset to a vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:13:58.682471Z",
     "start_time": "2018-10-17T17:13:58.673970Z"
    }
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "ft = count_vectorizer.fit_transform(docs_as_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's see the vocabulary. Every unique term in the corpus is assigned a position in the dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:13:58.693358Z",
     "start_time": "2018-10-17T17:13:58.685700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'enjoy': 2,\n",
       " 'like': 6,\n",
       " 'funny': 3,\n",
       " 'happy': 4,\n",
       " 'hate': 5,\n",
       " 'boring': 0,\n",
       " 'dull': 1}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.vocabulary_\n",
    "#[\"boring\",\"dull\",\"enjoy\",\"funny\",\"happy\",\"hate\",\"like\"]\n",
    "#\"enjoy like\"\n",
    "#[0,0,1,0,0,0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The positions are useful when finding if the word exists in the particular document. For example, if the column with index `2` has a value greater than `0` then the document will contain the word `enjoy`. \n",
    "\n",
    "**NOTE: ** The position of the words are random and hence the description does not fit the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:13:58.707065Z",
     "start_time": "2018-10-17T17:13:58.697298Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 1, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 1, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 1],\n",
       "       [1, 1, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Exploration - `nltk.corpus`\n",
    "\n",
    "`nltk.corpus` has many corpora (plural form of corpus) to allow you to download text to play with. The following are 2 more contemporary corpora. Before you access the corpus, ensure that you use `nltk.download()` to download the corpus to your local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">The Brown corpus is curated by Brown University. It has 1 million words in English and contains text from 500 sources. Each source is categorised into a genre.</alert>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:13:58.731105Z",
     "start_time": "2018-10-17T17:13:58.711513Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "try:\n",
    "    #Use corpus.categories() to show all category tags of each document.\n",
    "    print(brown.categories())\n",
    "except LookupError:\n",
    "    print(\"Downloading brown...\")    \n",
    "    nltk.download('brown')\n",
    "    print(\"Download brown complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">The Reuters Corpus contains 10,788 news documents totaling 1.3 million words. The documents have been classified into 90 topics.</alert>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:13:59.417108Z",
     "start_time": "2018-10-17T17:13:58.738972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U', '.', 'S', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import reuters\n",
    "try:\n",
    "    #Use corpus.words() to show all words that appear in the corpus.\n",
    "    print(reuters.words()[:20])\n",
    "except LookupError:\n",
    "    print(\"Downloading reuters...\")      \n",
    "    nltk.download('reuters')\n",
    "    print(\"Download reuters complete\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Credits**\n",
    "- Hackwagon\n",
    "- [sebleier](https://gist.github.com/sebleier/554280)\n",
    "- [Kaggle (Billboard 1964-2015 Songs + Lyrics)](https://www.kaggle.com/rakannimer/billboard-lyrics)\n",
    "- [Kaggle (Bag of Words Meets Bags of Popcorn)](https://www.kaggle.com/c/word2vec-nlp-tutorial/data)\n",
    "- [Kaggle (Top tracks of 2017)](https://www.kaggle.com/nadintamer/top-tracks-of-2017)\n",
    "- [Kaggle (Lending club loan data)](https://www.kaggle.com/wendykan/lending-club-loan-data)\n",
    "\n",
    "**Footnote**\n",
    "\n",
    "(1) : The reviews are partially processed. Only removal of special characters was performed. The remaining steps to be performed are stemming and removal of stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
